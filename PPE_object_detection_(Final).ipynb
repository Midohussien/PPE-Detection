{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PPE Object Detection ( Using Yolo7 )\n",
        "**CapStone Project**\n",
        "## ITAI-2277-Artificial Intel Resource.\n",
        "**Professor: G. Raymond Brown**\n",
        "### Team Members:\n",
        "1. Mohamed Mohamed.\n",
        "2. Sumesh Surendran.\n",
        "3. Cheyenne Hathaway.\n",
        "4. Bryanna Wallace\n"
      ],
      "metadata": {
        "id": "sso3PSNX1PH_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction :\n",
        "we will walk  through the process of training an object detection model that can detect people wearing a helmet. This project consists of below phases:\n",
        "1. Install YOLOv7 dependencies.\n",
        "2. Preparing dataset.\n",
        "3. Annotation converting from PASCAL VOC format to YOLO.\n",
        "5. Setup the YAML files for training\n",
        "6. Training the model\n",
        "7. Testing the model in images and videos\n"
      ],
      "metadata": {
        "id": "QJo4yW973t0t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1:\n",
        "Install YOLOv7 dependencies"
      ],
      "metadata": {
        "id": "rrsoKvP15HYD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aP-9_pMElLKy",
        "outputId": "0f7c1ade-f3f9-47da-c12e-7a3224d7a6fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cloning of the yolov7 official repository:\n",
        "!git clone https://github.com/WongKinYiu/yolov7"
      ],
      "metadata": {
        "id": "G6EYwcS31N5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/mido/yolov7"
      ],
      "metadata": {
        "id": "6RQ7iAU-DwuW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab5a30be-bb09-4bd0-b1ef-080710051dda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/mido/yolov7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# installing all the requirements libraries:\n",
        "!pip3 install -r requirements.txt"
      ],
      "metadata": {
        "id": "8lt4V-St1N1S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24ee4bce-5e4a-470d-8bd2-68d61ec91e6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (3.7.1)\n",
            "Requirement already satisfied: numpy<1.24.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.23.5)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (4.8.0.76)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (9.4.0)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (1.11.4)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision!=0.13.0,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (0.16.0+cu118)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (4.66.1)\n",
            "Requirement already satisfied: protobuf<4.21.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (3.20.3)\n",
            "Requirement already satisfied: tensorboard>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (2.14.1)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 21)) (1.5.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 22)) (0.12.2)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 34)) (7.34.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 35)) (5.9.5)\n",
            "Collecting thop (from -r requirements.txt (line 36))\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (2023.11.17)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (2.1.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (1.59.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (3.5.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (3.0.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 21)) (2023.3.post1)\n",
            "Collecting jedi>=0.16 (from ipython->-r requirements.txt (line 34))\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 34)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 34)) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 34)) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 34)) (3.0.41)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 34)) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 34)) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 34)) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 34)) (4.9.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 17)) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 17)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 17)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.4.1->-r requirements.txt (line 17)) (1.3.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->-r requirements.txt (line 34)) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->-r requirements.txt (line 34)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->-r requirements.txt (line 34)) (0.2.12)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.4.1->-r requirements.txt (line 17)) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 17)) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.4.1->-r requirements.txt (line 17)) (3.2.2)\n",
            "Installing collected packages: jedi, thop\n",
            "Successfully installed jedi-0.19.1 thop-0.1.1.post2209072238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the trained weights for yolo7:\n",
        "!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt"
      ],
      "metadata": {
        "id": "-PMw71E-1Nya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2:\n",
        "Preparing your own dataset:\n",
        "- we used the MakeMl Dataset ( we can use any other dataset from any resource but make sure that the annotations in the yolo7 format) .\n",
        "- The dataset includes images of the following class:\n",
        "1. Helmet\n",
        "2. Person\n",
        "3. Head."
      ],
      "metadata": {
        "id": "0LCHTBsu6OUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a directory in the same folder with yolo7 :\n",
        "import os\n",
        "\n",
        "if not os.path.isdir('DataSet'):\n",
        "  os.makedirs('DataSet')"
      ],
      "metadata": {
        "id": "FIby6a8j1Ntg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd DataSet"
      ],
      "metadata": {
        "id": "yYKNgP_21Nrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# downloading the dataset:\n",
        "!wget -O hardhatworkerspascalvoc.zip https://arcraftimages.s3-accelerate.amazonaws.com/Datasets/HardHatWorkers/HardHatWorkersPascalVOC.zip?region=us-east-2"
      ],
      "metadata": {
        "id": "xV2CLMwZ1Nnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip the dataset:\n",
        "unzip hardhatworkerspascalvoc.zip"
      ],
      "metadata": {
        "id": "i1VBWe7Q1Nlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3:\n",
        "Converting annotations from PASCAL-VOC to YOLOv7\n"
      ],
      "metadata": {
        "id": "jfsnsp079tyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a little script that does it for you:\n",
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "# Dictionary that maps class names to IDs\n",
        "class_name_to_id_mapping = {\"helmet\": 0,\n",
        "                           \"person\": 1,\n",
        "                           \"head\": 2}\n",
        "\n",
        "def extract_info_from_xml(xml_file):\n",
        "    root = ET.parse(xml_file).getroot()\n",
        "\n",
        "    # Initialise the info dict\n",
        "    info_dict = {}\n",
        "    info_dict['bboxes'] = []\n",
        "\n",
        "    # Parse the XML Tree\n",
        "    for elem in root:\n",
        "        # Get the file name\n",
        "        if elem.tag == \"filename\":\n",
        "            info_dict['filename'] = elem.text\n",
        "\n",
        "        # Get the image size\n",
        "        elif elem.tag == \"size\":\n",
        "            image_size = []\n",
        "            for subelem in elem:\n",
        "                image_size.append(int(subelem.text))\n",
        "\n",
        "            info_dict['image_size'] = tuple(image_size)\n",
        "\n",
        "        # Get details of the bounding box\n",
        "        elif elem.tag == \"object\":\n",
        "            bbox = {}\n",
        "            for subelem in elem:\n",
        "                if subelem.tag == \"name\":\n",
        "                    bbox[\"class\"] = subelem.text\n",
        "\n",
        "                elif subelem.tag == \"bndbox\":\n",
        "                    for subsubelem in subelem:\n",
        "                        bbox[subsubelem.tag] = int(subsubelem.text)\n",
        "            info_dict['bboxes'].append(bbox)\n",
        "\n",
        "    return info_dict\n",
        "\n",
        "\n",
        "# Convert the info dict to the required yolo format and write it to disk\n",
        "def convert_to_yolov5(info_dict):\n",
        "    print_buffer = []\n",
        "\n",
        "    # For each bounding box\n",
        "    for b in info_dict[\"bboxes\"]:\n",
        "        try:\n",
        "            class_id = class_name_to_id_mapping[b[\"class\"]]\n",
        "        except KeyError:\n",
        "            print(\"Invalid Class. Must be one from \", class_name_to_id_mapping.keys())\n",
        "\n",
        "        # Transform the bbox co-ordinates as per the format required by YOLO v5\n",
        "        b_center_x = (b[\"xmin\"] + b[\"xmax\"]) / 2\n",
        "        b_center_y = (b[\"ymin\"] + b[\"ymax\"]) / 2\n",
        "        b_width    = (b[\"xmax\"] - b[\"xmin\"])\n",
        "        b_height   = (b[\"ymax\"] - b[\"ymin\"])\n",
        "\n",
        "        # Normalise the co-ordinates by the dimensions of the image\n",
        "        image_w, image_h, image_c = info_dict[\"image_size\"]\n",
        "        b_center_x /= image_w\n",
        "        b_center_y /= image_h\n",
        "        b_width    /= image_w\n",
        "        b_height   /= image_h\n",
        "\n",
        "        #Write the bbox details to the file\n",
        "        print_buffer.append(\"{} {:.3f} {:.3f} {:.3f} {:.3f}\".format(class_id, b_center_x, b_center_y, b_width, b_height))\n",
        "\n",
        "    # Name of the file which we have to save\n",
        "    save_file_name = os.path.join(\"annotations\", info_dict[\"filename\"].replace(\"png\", \"txt\"))\n",
        "\n",
        "    # Save the annotation to disk\n",
        "    print(\"\\n\".join(print_buffer), file= open(save_file_name, \"w\"))\n",
        "\n",
        "annotations = [os.path.join('annotations', x) for x in os.listdir('annotations') if x[-3:] == \"xml\"]\n",
        "annotations.sort()\n",
        "\n",
        "# Convert and save the annotations\n",
        "for ann in tqdm(annotations):\n",
        "    info_dict = extract_info_from_xml(ann)\n",
        "    convert_to_yolov5(info_dict)\n",
        "annotations = [os.path.join('annotations', x) for x in os.listdir('annotations') if x[-3:] == \"txt\"]"
      ],
      "metadata": {
        "id": "HjFWUHGw1NhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### your project directory should be like :\n",
        "yolov7       \n",
        "└─── Dataset        \n",
        "    │\n",
        "    └─── images        \n",
        "            │\n",
        "            └─── test       \n",
        "            │\n",
        "            └─── train       \n",
        "            │\n",
        "            └─── val       \n",
        "    └──  labels       \n",
        "            │\n",
        "            └─── test       \n",
        "            │\n",
        "            └─── train       \n",
        "            │\n",
        "            └─── val       "
      ],
      "metadata": {
        "id": "y4N8dTwk_Fxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split dataset:\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shutil\n",
        "# Read images and annotations\n",
        "images = [os.path.join('images', x) for x in os.listdir('images')]\n",
        "annotations = [os.path.join('annotations', x) for x in os.listdir('annotations') if x[-3:] == \"txt\"]\n",
        "\n",
        "images.sort()\n",
        "annotations.sort()\n",
        "\n",
        "# Split the dataset into train-valid-test splits\n",
        "train_images, val_images, train_annotations, val_annotations = train_test_split(images, annotations, test_size = 0.2, random_state = 1)\n",
        "val_images, test_images, val_annotations, test_annotations = train_test_split(val_images, val_annotations, test_size = 0.5, random_state = 1)\n",
        "\n",
        "#Utility function to move images\n",
        "def move_files_to_folder(list_of_files, destination_folder):\n",
        "    for f in list_of_files:\n",
        "        try:\n",
        "            shutil.move(f, destination_folder)\n",
        "        except:\n",
        "            print(f)\n",
        "            assert False\n",
        "\n",
        "# Move the splits into their folders\n",
        "move_files_to_folder(train_images, 'newimages/train')\n",
        "move_files_to_folder(val_images, 'newimages/val/')\n",
        "move_files_to_folder(test_images, 'newimages/test/')\n",
        "move_files_to_folder(train_annotations, 'newannotations/train/')\n",
        "move_files_to_folder(val_annotations, 'newannotations/val/')\n",
        "move_files_to_folder(test_annotations, 'newannotations/test/')"
      ],
      "metadata": {
        "id": "h4Bu9coA1Nfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4:\n",
        "Setup the YAML files for training:     "
      ],
      "metadata": {
        "id": "knNjT74EAe86"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- create a new YAML file with this data:     \n",
        "train: /home/jetson/yolov7/dataset/images/train/   #(train images path)     \n",
        "val:  /home/jetson/yolov7/dataset/images/val/      #(validating images path)     \n",
        "test: /home/jetson/yolov7/dataset/images/test/      #(test images path)     \n",
        "- number of classes     \n",
        "nc: 3     \n",
        "-  class names     \n",
        "names: [\"helmet\",\"person\", \"head\"]"
      ],
      "metadata": {
        "id": "AlE4RJEDBW5m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## step 5:\n",
        "Training the model:     \n",
        "**Notes before training:**\n",
        "1. Make sure your data YAML file has the right bath for your data.\n",
        "2. change the number of classes in the cfg YAML file to 3.\n"
      ],
      "metadata": {
        "id": "sWMZZVaSBsvf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8iZfM1gpjpm",
        "outputId": "0e24f578-4f6a-47b8-f90c-172066c02038"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-20 22:10:53.375459: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-20 22:10:53.375513: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-20 22:10:53.375538: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-20 22:10:53.383563: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-20 22:10:54.333682: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "YOLOR 🚀 v0.1-128-ga207844 torch 2.1.0+cu118 CUDA:0 (Tesla T4, 15101.8125MB)\n",
            "\n",
            "Namespace(weights='yolov7x (1).pt', cfg='cfg/training/yolov7x.yaml', data='data/coco.yaml', hyp='data/hyp.scratch.custom.yaml', epochs=15, batch_size=16, img_size=[640, 640], rect=False, resume=False, nosave=False, notest=False, noautoanchor=False, evolve=False, bucket='', cache_images=False, image_weights=False, device='0', multi_scale=False, single_cls=False, adam=False, sync_bn=False, local_rank=-1, workers=8, project='runs/train', entity=None, name='yolov7x_custom', exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, upload_dataset=False, bbox_interval=-1, save_period=-1, artifact_alias='latest', freeze=[0], v5_metric=False, world_size=1, global_rank=-1, save_dir='runs/train/yolov7x_custom2', total_batch_size=16)\n",
            "\u001b[34m\u001b[1mtensorboard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.3, cls_pw=1.0, obj=0.7, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.2, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, paste_in=0.0, loss_ota=1\n",
            "\u001b[34m\u001b[1mwandb: \u001b[0mInstall Weights & Biases for YOLOR logging with 'pip install wandb' (recommended)\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      1160  models.common.Conv                      [3, 40, 3, 1]                 \n",
            "  1                -1  1     28960  models.common.Conv                      [40, 80, 3, 2]                \n",
            "  2                -1  1     57760  models.common.Conv                      [80, 80, 3, 1]                \n",
            "  3                -1  1    115520  models.common.Conv                      [80, 160, 3, 2]               \n",
            "  4                -1  1     10368  models.common.Conv                      [160, 64, 1, 1]               \n",
            "  5                -2  1     10368  models.common.Conv                      [160, 64, 1, 1]               \n",
            "  6                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            "  7                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            "  8                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            "  9                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            " 10                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            " 11                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            " 12[-1, -3, -5, -7, -8]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    103040  models.common.Conv                      [320, 320, 1, 1]              \n",
            " 14                -1  1         0  models.common.MP                        []                            \n",
            " 15                -1  1     51520  models.common.Conv                      [320, 160, 1, 1]              \n",
            " 16                -3  1     51520  models.common.Conv                      [320, 160, 1, 1]              \n",
            " 17                -1  1    230720  models.common.Conv                      [160, 160, 3, 2]              \n",
            " 18          [-1, -3]  1         0  models.common.Concat                    [1]                           \n",
            " 19                -1  1     41216  models.common.Conv                      [320, 128, 1, 1]              \n",
            " 20                -2  1     41216  models.common.Conv                      [320, 128, 1, 1]              \n",
            " 21                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 22                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 23                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 24                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 25                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 26                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 27[-1, -3, -5, -7, -8]  1         0  models.common.Concat                    [1]                           \n",
            " 28                -1  1    410880  models.common.Conv                      [640, 640, 1, 1]              \n",
            " 29                -1  1         0  models.common.MP                        []                            \n",
            " 30                -1  1    205440  models.common.Conv                      [640, 320, 1, 1]              \n",
            " 31                -3  1    205440  models.common.Conv                      [640, 320, 1, 1]              \n",
            " 32                -1  1    922240  models.common.Conv                      [320, 320, 3, 2]              \n",
            " 33          [-1, -3]  1         0  models.common.Concat                    [1]                           \n",
            " 34                -1  1    164352  models.common.Conv                      [640, 256, 1, 1]              \n",
            " 35                -2  1    164352  models.common.Conv                      [640, 256, 1, 1]              \n",
            " 36                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 37                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 38                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 39                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 40                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 41                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 42[-1, -3, -5, -7, -8]  1         0  models.common.Concat                    [1]                           \n",
            " 43                -1  1   1640960  models.common.Conv                      [1280, 1280, 1, 1]            \n",
            " 44                -1  1         0  models.common.MP                        []                            \n",
            " 45                -1  1    820480  models.common.Conv                      [1280, 640, 1, 1]             \n",
            " 46                -3  1    820480  models.common.Conv                      [1280, 640, 1, 1]             \n",
            " 47                -1  1   3687680  models.common.Conv                      [640, 640, 3, 2]              \n",
            " 48          [-1, -3]  1         0  models.common.Concat                    [1]                           \n",
            " 49                -1  1    328192  models.common.Conv                      [1280, 256, 1, 1]             \n",
            " 50                -2  1    328192  models.common.Conv                      [1280, 256, 1, 1]             \n",
            " 51                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 52                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 53                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 54                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 55                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 56                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 57[-1, -3, -5, -7, -8]  1         0  models.common.Concat                    [1]                           \n",
            " 58                -1  1   1640960  models.common.Conv                      [1280, 1280, 1, 1]            \n",
            " 59                -1  1  11887360  models.common.SPPCSPC                   [1280, 640, 1]                \n",
            " 60                -1  1    205440  models.common.Conv                      [640, 320, 1, 1]              \n",
            " 61                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 62                43  1    410240  models.common.Conv                      [1280, 320, 1, 1]             \n",
            " 63          [-1, -2]  1         0  models.common.Concat                    [1]                           \n",
            " 64                -1  1    164352  models.common.Conv                      [640, 256, 1, 1]              \n",
            " 65                -2  1    164352  models.common.Conv                      [640, 256, 1, 1]              \n",
            " 66                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 67                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 68                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 69                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 70                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 71                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 72[-1, -3, -5, -7, -8]  1         0  models.common.Concat                    [1]                           \n",
            " 73                -1  1    410240  models.common.Conv                      [1280, 320, 1, 1]             \n",
            " 74                -1  1     51520  models.common.Conv                      [320, 160, 1, 1]              \n",
            " 75                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 76                28  1    102720  models.common.Conv                      [640, 160, 1, 1]              \n",
            " 77          [-1, -2]  1         0  models.common.Concat                    [1]                           \n",
            " 78                -1  1     41216  models.common.Conv                      [320, 128, 1, 1]              \n",
            " 79                -2  1     41216  models.common.Conv                      [320, 128, 1, 1]              \n",
            " 80                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 81                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 82                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 83                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 84                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 85                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 86[-1, -3, -5, -7, -8]  1         0  models.common.Concat                    [1]                           \n",
            " 87                -1  1    102720  models.common.Conv                      [640, 160, 1, 1]              \n",
            " 88                -1  1         0  models.common.MP                        []                            \n",
            " 89                -1  1     25920  models.common.Conv                      [160, 160, 1, 1]              \n",
            " 90                -3  1     25920  models.common.Conv                      [160, 160, 1, 1]              \n",
            " 91                -1  1    230720  models.common.Conv                      [160, 160, 3, 2]              \n",
            " 92      [-1, -3, 73]  1         0  models.common.Concat                    [1]                           \n",
            " 93                -1  1    164352  models.common.Conv                      [640, 256, 1, 1]              \n",
            " 94                -2  1    164352  models.common.Conv                      [640, 256, 1, 1]              \n",
            " 95                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 96                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 97                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 98                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 99                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            "100                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            "101[-1, -3, -5, -7, -8]  1         0  models.common.Concat                    [1]                           \n",
            "102                -1  1    410240  models.common.Conv                      [1280, 320, 1, 1]             \n",
            "103                -1  1         0  models.common.MP                        []                            \n",
            "104                -1  1    103040  models.common.Conv                      [320, 320, 1, 1]              \n",
            "105                -3  1    103040  models.common.Conv                      [320, 320, 1, 1]              \n",
            "106                -1  1    922240  models.common.Conv                      [320, 320, 3, 2]              \n",
            "107      [-1, -3, 59]  1         0  models.common.Concat                    [1]                           \n",
            "108                -1  1    656384  models.common.Conv                      [1280, 512, 1, 1]             \n",
            "109                -2  1    656384  models.common.Conv                      [1280, 512, 1, 1]             \n",
            "110                -1  1   2360320  models.common.Conv                      [512, 512, 3, 1]              \n",
            "111                -1  1   2360320  models.common.Conv                      [512, 512, 3, 1]              \n",
            "112                -1  1   2360320  models.common.Conv                      [512, 512, 3, 1]              \n",
            "113                -1  1   2360320  models.common.Conv                      [512, 512, 3, 1]              \n",
            "114                -1  1   2360320  models.common.Conv                      [512, 512, 3, 1]              \n",
            "115                -1  1   2360320  models.common.Conv                      [512, 512, 3, 1]              \n",
            "116[-1, -3, -5, -7, -8]  1         0  models.common.Concat                    [1]                           \n",
            "117                -1  1   1639680  models.common.Conv                      [2560, 640, 1, 1]             \n",
            "118                87  1    461440  models.common.Conv                      [160, 320, 3, 1]              \n",
            "119               102  1   1844480  models.common.Conv                      [320, 640, 3, 1]              \n",
            "120               117  1   7375360  models.common.Conv                      [640, 1280, 3, 1]             \n",
            "121   [118, 119, 120]  1     76358  models.yolo.IDetect                     [6, [[12, 16, 19, 36, 40, 28], [36, 75, 76, 55, 72, 146], [142, 110, 192, 243, 459, 401]], [320, 640, 1280]]\n",
            "Model Summary: 467 layers, 70848782 parameters, 70848782 gradients\n",
            "\n",
            "Transferred 630/644 items from yolov7x (1).pt\n",
            "Scaled weight_decay = 0.0005\n",
            "Optimizer groups: 108 .bias, 108 conv.weight, 111 other\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/gdrive/MyDrive/mido_2/yolov7/PPE-detection-3/labels/train' images and labels... 404 found, 0 missing, 0 empty, 0 corrupted:  36% 404/1125 [03:34<05:29,  2.19it/s]\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/gdrive/MyDrive/mido_2/yolov7/PPE-detection-3/images/train/Video2_167_jpg.rf.db1a257b176d31861c444d4ecb19e81e.jpg: duplicate labels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/gdrive/MyDrive/mido_2/yolov7/PPE-detection-3/labels/train' images and labels... 405 found, 0 missing, 0 empty, 1 corrupted:  36% 405/1125 [03:34<05:27,  2.20it/s]\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label /content/gdrive/MyDrive/mido_2/yolov7/PPE-detection-3/images/train/Video2_167_jpg.rf.fbe0cc824f6762c8ef7c28b5829cb033.jpg: duplicate labels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/gdrive/MyDrive/mido_2/yolov7/PPE-detection-3/labels/train' images and labels... 1125 found, 0 missing, 0 empty, 2 corrupted: 100% 1125/1125 [09:23<00:00,  2.00it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/gdrive/MyDrive/mido_2/yolov7/PPE-detection-3/labels/train.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/gdrive/MyDrive/mido_2/yolov7/PPE-detection-3/labels/val' images and labels... 332 found, 0 missing, 0 empty, 0 corrupted: 100% 332/332 [02:41<00:00,  2.06it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/gdrive/MyDrive/mido_2/yolov7/PPE-detection-3/labels/val.cache\n",
            "\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 5.06, Best Possible Recall (BPR) = 0.9996\n",
            "Image sizes 640 train, 640 test\n",
            "Using 8 dataloader workers\n",
            "Logging results to runs/train/yolov7x_custom2\n",
            "Starting training for 15 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      0/14     2.34G   0.06246   0.02479   0.02597    0.1132        24       640: 100% 71/71 [01:59<00:00,  1.68s/it]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95:   0% 0/11 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:23<00:00,  2.13s/it]\n",
            "                 all         332        1713       0.183       0.275       0.203      0.0809\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      1/14     14.2G   0.05043   0.02208   0.01326   0.08576        30       640: 100% 71/71 [01:30<00:00,  1.27s/it]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:10<00:00,  1.07it/s]\n",
            "                 all         332        1713       0.657       0.447       0.387       0.126\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      2/14     6.98G   0.05113    0.0167  0.009041   0.07688        22       640: 100% 71/71 [01:29<00:00,  1.26s/it]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:09<00:00,  1.11it/s]\n",
            "                 all         332        1713       0.758       0.482       0.467         0.2\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      3/14     14.4G   0.04738   0.01608  0.007703   0.07117        24       640: 100% 71/71 [01:28<00:00,  1.25s/it]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:09<00:00,  1.12it/s]\n",
            "                 all         332        1713       0.703       0.503       0.429       0.143\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      4/14     14.4G   0.04241   0.01463  0.006386   0.06342        28       640: 100% 71/71 [01:28<00:00,  1.25s/it]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:09<00:00,  1.12it/s]\n",
            "                 all         332        1713        0.86       0.555       0.562       0.325\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      5/14     14.4G   0.03785   0.01484  0.005264   0.05795        28       640: 100% 71/71 [01:28<00:00,  1.25s/it]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:09<00:00,  1.13it/s]\n",
            "                 all         332        1713       0.864        0.58        0.59        0.29\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      6/14     14.4G   0.03425   0.01378  0.004554   0.05259        29       640: 100% 71/71 [01:28<00:00,  1.25s/it]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:09<00:00,  1.13it/s]\n",
            "                 all         332        1713       0.925       0.608       0.626       0.372\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      7/14     14.4G   0.03497   0.01293  0.004471   0.05237        22       640: 100% 71/71 [01:28<00:00,  1.25s/it]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:09<00:00,  1.13it/s]\n",
            "                 all         332        1713       0.941       0.593        0.63       0.382\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      8/14     5.62G   0.03127   0.01233  0.003916   0.04751        23       640: 100% 71/71 [01:28<00:00,  1.25s/it]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:09<00:00,  1.13it/s]\n",
            "                 all         332        1713       0.931        0.62       0.639       0.405\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      9/14     14.4G   0.02787   0.01207  0.003276   0.04321        12       640: 100% 71/71 [01:28<00:00,  1.25s/it]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:09<00:00,  1.14it/s]\n",
            "                 all         332        1713       0.939       0.619       0.645       0.426\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     10/14     14.3G   0.02578   0.01141  0.003122   0.04031        14       640: 100% 71/71 [01:28<00:00,  1.24s/it]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:09<00:00,  1.14it/s]\n",
            "                 all         332        1713       0.951       0.617       0.653       0.464\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     11/14     14.3G   0.02451   0.01095  0.002552   0.03801        26       640: 100% 71/71 [01:28<00:00,  1.24s/it]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:09<00:00,  1.12it/s]\n",
            "                 all         332        1713       0.963       0.616       0.658       0.451\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     12/14     14.4G   0.02362   0.01068  0.002639   0.03694        22       640: 100% 71/71 [01:28<00:00,  1.24s/it]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:09<00:00,  1.14it/s]\n",
            "                 all         332        1713       0.959        0.62       0.688       0.485\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     13/14     14.4G   0.02165    0.0103  0.002309   0.03426        10       640: 100% 71/71 [01:28<00:00,  1.24s/it]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:09<00:00,  1.14it/s]\n",
            "                 all         332        1713       0.965        0.62       0.717       0.499\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     14/14     14.4G   0.02062   0.01014  0.002145    0.0329        20       640: 100% 71/71 [01:28<00:00,  1.25s/it]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 11/11 [00:10<00:00,  1.03it/s]\n",
            "                 all         332        1713       0.847       0.688       0.724       0.508\n",
            "              Helmet         332         281       0.856       0.972       0.965        0.65\n",
            "              Person         332         306       0.665        0.98       0.963       0.795\n",
            "        Safety_shoes         332         557       0.816       0.943        0.96       0.664\n",
            "                Vest         332         404       0.827        0.99       0.991       0.767\n",
            "           no-helmet         332         136       0.915       0.243       0.465       0.172\n",
            "             no-vest         332          29           1           0     0.00292     0.00122\n",
            "15 epochs completed in 0.441 hours.\n",
            "\n",
            "Optimizer stripped from runs/train/yolov7x_custom2/weights/last.pt, 142.2MB\n",
            "Optimizer stripped from runs/train/yolov7x_custom2/weights/best.pt, 142.2MB\n"
          ]
        }
      ],
      "source": [
        "!python train.py --device 0 --batch-size 16 --epochs 15 --img 640 640 --data data/coco.yaml --hyp data/hyp.scratch.custom.yaml --cfg cfg/training/yolov7x.yaml --weights \"yolov7x (1).pt\" --name yolov7x_custom"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## step 6:\n",
        "Testing the model in images and videos"
      ],
      "metadata": {
        "id": "pFERyIj3DMri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 detect.py --weights \"runs/train/yolov7x_custom2/weights/best.pt\"  --conf 0.5 --img-size 640 --source \"1.jpg\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiJaSKYjcl4g",
        "outputId": "f5548fd2-e912-4d6d-d2c3-fa6f75835723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(weights=['runs/train/yolov7x_custom2/weights/best.pt'], source='1.jpg', img_size=640, conf_thres=0.5, iou_thres=0.45, device='', view_img=False, save_txt=False, save_conf=False, nosave=False, classes=None, agnostic_nms=False, augment=False, update=False, project='runs/detect', name='exp', exist_ok=False, no_trace=False)\n",
            "YOLOR 🚀 v0.1-128-ga207844 torch 2.1.0+cu118 CUDA:0 (Tesla T4, 15101.8125MB)\n",
            "\n",
            "Fusing layers... \n",
            "IDetect.fuse\n",
            "Model Summary: 362 layers, 70816134 parameters, 0 gradients\n",
            " Convert model to Traced-model... \n",
            " traced_script_module saved! \n",
            " model is traced! \n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "1 Helmet, 2 Persons, 2 Vests, Done. (36.3ms) Inference, (1.6ms) NMS\n",
            " The image with the result is saved in: runs/detect/exp/1.jpg\n",
            "Done. (0.074s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## step 7:\n",
        "Evaluating the Model"
      ],
      "metadata": {
        "id": "4YLWKJ0kKb4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python test.py --weights best.pt --task test --data data/custom_data.yaml"
      ],
      "metadata": {
        "id": "Evaxojq5-J9Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4912a071-38dd-44d0-ec18-43fffcb4a67a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(weights=['best.pt'], data='data/custom_data.yaml', batch_size=32, img_size=640, conf_thres=0.001, iou_thres=0.65, task='test', device='', single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project='runs/test', name='exp', exist_ok=False, no_trace=False, v5_metric=False)\n",
            "YOLOR 🚀 v0.1-128-ga207844 torch 2.1.0+cu118 CUDA:0 (Tesla T4, 15101.8125MB)\n",
            "\n",
            "Fusing layers... \n",
            "IDetect.fuse\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Model Summary: 362 layers, 70795920 parameters, 0 gradients, 188.0 GFLOPS\n",
            " Convert model to Traced-model... \n",
            " traced_script_module saved! \n",
            " model is traced! \n",
            "\n",
            "\n",
            "WARNING: Dataset not found, nonexistent paths: ['/content/gdrive/MyDrive/mido/yolov7/small_data/images/val']\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/mido/yolov7/test.py\", line 319, in <module>\n",
            "    test(opt.data,\n",
            "  File \"/content/drive/MyDrive/mido/yolov7/test.py\", line 76, in test\n",
            "    check_dataset(data)  # check\n",
            "  File \"/content/drive/MyDrive/mido/yolov7/utils/general.py\", line 173, in check_dataset\n",
            "    raise Exception('Dataset not found.')\n",
            "Exception: Dataset not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## step 8:\n",
        "Running the model on real time camera streaming"
      ],
      "metadata": {
        "id": "1JReOCY6ESdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 detect.py --weights \"runs/train/yolov7x_custom2/weights/best.pt\"  --conf 0.5 --img-size 640 --source 0 --no-trace"
      ],
      "metadata": {
        "id": "R1qpNBJdEqzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resources:\n",
        "1. https://www.hackster.io/shahizat/safety-helmet-detection-system-based-on-yolov7-algorithm-3d4cef\n",
        "2. https://www.kaggle.com/datasets/andrewmvd/hard-hat-detection (another link for the dataset)\n",
        "3. https://blog.paperspace.com/train-yolov5-custom-data/\n",
        "4. https://www.youtube.com/watch?v=XzUMigbYRUI"
      ],
      "metadata": {
        "id": "VUG4L29Z2648"
      }
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}